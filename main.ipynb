{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save the book into text and try to get all the unique characters from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', '*', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '\\ufeff']\n"
     ]
    }
   ],
   "source": [
    "with open(\"wizard_of_oz.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "chars = sorted(set(text))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then build a simple encoder and decoder to convert any word into a set of numbers representing that word. In this simple case, we just attach each character to its index, and when we have a word \"hello\", we just look it up and see ok h = index 63, e = index 60...etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_to_int = {character:index for index, character in enumerate(chars)}\n",
    "int_to_string = {index:character for index, character in enumerate(chars)}\n",
    "encoder = lambda text: [string_to_int[character] for character in text]\n",
    "decoder = lambda list_num: \"\".join(int_to_string[num] for num in list_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we encode the entire book (text). For ease of manipulation and do all the maths, we need to change the data strucutre. PyTorch has the tensor data structure which can be easily manipulated with maths. Thus, we convert our normal list of numbers into a tensor (multi-dimension array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([80,  1,  1, 28, 39, 42, 39, 44, 32, 49,  1, 25, 38, 28,  1, 44, 32, 29,\n",
      "         1, 47, 33, 50, 25, 42, 28,  1, 33, 38,  1, 39, 50,  0,  0,  1,  1, 26,\n",
      "        49,  0,  0,  1,  1, 36, 11,  1, 30, 42, 25, 38, 35,  1, 26, 25, 45, 37,\n",
      "         0,  0,  1,  1, 25, 45, 44, 32, 39, 42,  1, 39, 30,  1, 44, 32, 29,  1,\n",
      "        47, 33, 50, 25, 42, 28,  1, 39, 30,  1, 39, 50,  9,  1, 44, 32, 29,  1,\n",
      "        36, 25, 38, 28,  1, 39, 30,  1, 39, 50])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data = torch.tensor(encoder(text), dtype=torch.long)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split up the data into: training and validation to avoid overfitting\n",
    "setting the block size to 8 characters and creating a sliding window x and y\n",
    "this mimic how the model guess the next character given the previous character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([80]) target is tensor(1)\n",
      "when input is tensor([80,  1]) target is tensor(1)\n",
      "when input is tensor([80,  1,  1]) target is tensor(28)\n",
      "when input is tensor([80,  1,  1, 28]) target is tensor(39)\n",
      "when input is tensor([80,  1,  1, 28, 39]) target is tensor(42)\n",
      "when input is tensor([80,  1,  1, 28, 39, 42]) target is tensor(39)\n",
      "when input is tensor([80,  1,  1, 28, 39, 42, 39]) target is tensor(44)\n",
      "when input is tensor([80,  1,  1, 28, 39, 42, 39, 44]) target is tensor(32)\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data)*0.8)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "block_size = 8\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1 : block_size+1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(\"when input is\", context, \"target is\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the problem with the above setup is scalability. It is \"sequential\" (ie. it is run in order by CPU). It can't be run all at once, which is what GPU can do.\n",
    "The first cell is for Window. The second cell is for using Apple Sillicon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Picking device and speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device_on_window = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device_on_window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "device = torch.device(\"mps\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's do some testing to see how fast GPU comparing to CPU. As we can see, when it comes to large operation, GPU is 100x faster than CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elapsed time is 0.27686405\n",
      "elapsed time is 33.59036207\n"
     ]
    }
   ],
   "source": [
    "a = 150\n",
    "torch_rand1 = torch.rand(a, a, a, a).to(device)\n",
    "toche_rand2 = torch.rand(a, a, a, a).to(device)\n",
    "np_rand1 = torch.rand(a, a, a, a)\n",
    "np_rand2 = torch.rand(a, a, a, a)\n",
    "\n",
    "start_time = time.time()\n",
    "rand = (torch_rand1 @ toche_rand2)\n",
    "endtime = time.time()\n",
    "elapsed_time = endtime - start_time\n",
    "print(f\"elapsed time is {elapsed_time:.8f}\")\n",
    "\n",
    "start_time = time.time()\n",
    "rand = np.multiply(np_rand1, np_rand2)\n",
    "endtime = time.time()\n",
    "elapsed_time = endtime - start_time\n",
    "print(f\"elapsed time is {elapsed_time:.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we train the model?\n",
    "We will continuously feed them a set of 8 characters, once at a time. The first sequence (x) is 1st to 8th (block_size), then we show them the result (y) --> that's how we \"teach\" them.\n",
    "ix is just a set of random positions that we will creating a sequence (set). We set the batch = 4, it means we will feed the model 4 set of x , then it will try to predict the next character, then we feed it 4 set of y, which is like the \"answer\". Then we do the comparision between the model's prediction and the answer. The difference is the loss function that we try to minimize. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([99040,  2156, 12509, 85304])\n",
      "input:\n",
      "tensor([[54, 73,  1, 73, 61, 58,  1, 31],\n",
      "        [ 1, 76, 62, 73, 61, 68, 74, 73],\n",
      "        [ 0,  0, 44, 61, 58,  1, 72, 64],\n",
      "        [78,  1, 59, 71, 68, 66,  1, 73]], device='mps:0')\n",
      "target:\n",
      "tensor([[73,  1, 73, 61, 58,  1, 31, 54],\n",
      "        [76, 62, 73, 61, 68, 74, 73,  1],\n",
      "        [ 0, 44, 61, 58,  1, 72, 64, 78],\n",
      "        [ 1, 59, 71, 68, 66,  1, 73, 61]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "n = int(len(data)*0.8)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "block_size = 8\n",
    "batch_size = 4\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == \"train\" else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    print(ix)\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x , y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "x, y = get_batch(\"train\")\n",
    "print(\"input:\")\n",
    "print(x)\n",
    "print(\"target:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
